@inproceedings{10.1145/3360322.3360992,
author = {Lahariya, Manu and Sadeghianpourhamami, Nasrin and Develder, Chris},
title = {Reduced State Space and Cost Function in Reinforcement Learning for Demand Response Control of Multiple EV Charging Stations},
year = {2019},
isbn = {9781450370059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360322.3360992},
doi = {10.1145/3360322.3360992},
abstract = {Electric vehicle (EV) charging stations represent a substantial load with significant flexibility. Balancing such load with model-free demand response (DR) based on reinforcement learning (RL) is an attractive approach. We build on previous RL research using a Markov decision process (MDP) to simultaneously coordinate multiple charging stations. The previously proposed approach is computationally expensive in terms of large training times, limiting its feasibility and practicality. We propose to a priori force the control policy to always fulfill any charging demand that does not offer any flexibility at a given point, and thus use an updated cost function. We compare the policy of the newly proposed approach with the original (costly) one, for the case of load flattening, in terms of (i) processing time to learn the RL-based charging policy, and (ii) overall performance of the policy decisions in terms of meeting the target load for unseen test data.},
booktitle = {Proceedings of the 6th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {344â€“345},
numpages = {2},
keywords = {reinforcement learning, Smart grid, smart charging, demand response, electric vehicle, Markov decision process},
location = {New York, NY, USA},
series = {BuildSys '19}
}